import pandas as pd
from difflib import get_close_matches
from transformers import pipeline, AutoTokenizer

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
MAX_NEW_TOKENS = 120
TEMPERATURE = 0.4

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---
def load_data(file_path: str):
    """–ß–∏—Ç–∞–µ—Ç Excel –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ ‚Üí —Ñ–∞–∫—Ç—ã."""
    df = pd.read_excel(file_path, sheet_name="–õ–∏—Å—Ç1", engine="openpyxl")
    df = df.fillna("–Ω–µ —É–∫–∞–∑–∞–Ω–æ")
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å: { "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": { "–±–∞–ª–ª—ã": ..., "–ø—Ä–µ–¥–º–µ—Ç—ã": ..., ... } }
    facts_map = {}
    for _, row in df.iterrows():
        facts_map[row["–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"]] = {
            "–±–∞–ª–ª—ã": row["–ü—Ä–æ—Ö–æ–¥–Ω–æ–π –±–∞–ª–ª –ï–ì–≠"],
            "–ø—Ä–µ–¥–º–µ—Ç—ã": row["–õ—é–±–∏–º—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã"],
            "–ø—Ä–æ—Ñ–µ—Å—Å–∏–∏": row["–ë—É–¥—É—â–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏"],
            "—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è": row["–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"]
        }
    return list(facts_map.keys()), facts_map

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ ---
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    gen = pipeline(
        "text-generation",
        model=MODEL_NAME,
        tokenizer=tokenizer,
        device="cpu",
        torch_dtype="auto"
    )
    return gen, tokenizer

# --- –ü–æ–∏—Å–∫ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å fuzzy-match ---
def extract_direction(question: str, directions: list[str]) -> str | None:
    q = question.lower()
    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É q: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫ –∏ —Ç.–ø.
    match = get_close_matches(q, directions, n=1, cutoff=0.4)
    return match[0] if match else None

# --- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏ –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ ---
def generate_response(
    gen_pipeline,
    tokenizer,
    direction: str,
    facts: dict,
    question: str
) -> str:
    # –°–æ–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é —Å—Ç—Ä–æ–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏
    facts_str = (
        f"–ü—Ä–æ—Ö–æ–¥–Ω–æ–π –±–∞–ª–ª: {facts['–±–∞–ª–ª—ã']}. "
        f"–ü—Ä–µ–¥–º–µ—Ç—ã: {facts['–ø—Ä–µ–¥–º–µ—Ç—ã']}. "
        f"–ü—Ä–æ—Ñ–µ—Å—Å–∏–∏: {facts['–ø—Ä–æ—Ñ–µ—Å—Å–∏–∏']}. "
        f"–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: {facts['—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è']}."
    )
    prompt = (
        f"–¢—ã ‚Äî –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø—Ä–∏—ë–º–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ –ò–†–ò–¢-–†–¢–§.\n"
        f"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {direction}\n"
        f"{facts_str}\n"
        f"–í–æ–ø—Ä–æ—Å: {question}\n"
        f"–û—Ç–≤–µ—Ç—å —á—ë—Ç–∫–æ –∏ –∫—Ä–∞—Ç–∫–æ, 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. "
        f"–ù–µ —É–ø–æ–º–∏–Ω–∞–π, —á—Ç–æ —Ç—ã ‚Äî –º–æ–¥–µ–ª—å –ò–ò."
    )

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç, –ø–µ—Ä–µ–¥–∞–≤–∞—è —Ç–æ–ª—å–∫–æ prompt –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    out = gen_pipeline(
        prompt,
        max_new_tokens=MAX_NEW_TOKENS,
        temperature=TEMPERATURE,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        return_full_text=False
    )
    # –ë–µ—Ä—ë–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ —á—É—Ç—å-—á—É—Ç—å –µ–≥–æ ¬´–ø–æ—á–∏—Å—Ç–∏–º¬ª
    answer = out[0]["generated_text"].strip()
    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–∂–Ω–æ –∑–¥–µ—Å—å –≤—ã–∑–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é clean_response(answer)
    return answer

# --- –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª–∏ ---
if __name__ == "__main__":
    directions, facts_map = load_data("–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è.xlsx")
    gen_pipeline, tokenizer = load_model()

    while True:
        q = input("üéì –í–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ ¬´–≤—ã—Ö–æ–¥¬ª): ").strip()
        if q.lower() == "–≤—ã—Ö–æ–¥":
            break
        dir_found = extract_direction(q, directions)
        if not dir_found:
            print("‚ùå –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            continue
        answer = generate_response(
            gen_pipeline,
            tokenizer,
            dir_found,
            facts_map[dir_found],
            q
        )
        print("üí° –û—Ç–≤–µ—Ç:", answer)